{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzZy2YCKkSgM",
        "outputId": "d52010b3-aedd-44fd-c2c4-c43f75e39ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] loss: 34.7022, Accuracy: 58.18%\n",
            "Epoch [2/20] loss: 23.3214, Accuracy: 76.33%\n",
            "Epoch [3/20] loss: 16.1527, Accuracy: 79.98%\n",
            "Epoch [4/20] loss: 11.2571, Accuracy: 82.29%\n",
            "Epoch [5/20] loss: 7.8833, Accuracy: 84.01%\n",
            "Epoch [6/20] loss: 5.5610, Accuracy: 85.35%\n",
            "Epoch [7/20] loss: 3.9527, Accuracy: 86.80%\n",
            "Epoch [8/20] loss: 2.8440, Accuracy: 87.94%\n",
            "Epoch [9/20] loss: 2.0734, Accuracy: 88.91%\n",
            "Epoch [10/20] loss: 1.5469, Accuracy: 89.45%\n",
            "Epoch [11/20] loss: 1.1745, Accuracy: 90.32%\n",
            "Epoch [12/20] loss: 0.9183, Accuracy: 90.80%\n",
            "Epoch [13/20] loss: 0.7428, Accuracy: 91.30%\n",
            "Epoch [14/20] loss: 0.6183, Accuracy: 91.78%\n",
            "Epoch [15/20] loss: 0.5366, Accuracy: 91.95%\n",
            "Epoch [16/20] loss: 0.4799, Accuracy: 92.04%\n",
            "Epoch [17/20] loss: 0.4393, Accuracy: 92.22%\n",
            "Epoch [18/20] loss: 0.4160, Accuracy: 92.33%\n",
            "Epoch [19/20] loss: 0.4022, Accuracy: 92.32%\n",
            "Epoch [20/20] loss: 0.3877, Accuracy: 92.41%\n",
            "Validation loss: 0.2999, accuracy: 95.07%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
        "        self.bias1 = torch.randn(1, hidden_size, requires_grad=True)\n",
        "        self.weights2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
        "        self.bias2 = torch.randn(1, output_size, requires_grad=True)\n",
        "        self.training = True  # Initialize the mode as 'training'\n",
        "        self.dropout_prob = 0.5\n",
        "        self.l2_lambda = 0.001\n",
        "\n",
        "        # Batch normalization parameters\n",
        "        self.bn1_running_mean = torch.zeros(1, hidden_size)\n",
        "        self.bn1_running_var = torch.ones(1, hidden_size)\n",
        "        self.bn1_mean = 0\n",
        "        self.bn1_var = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1 with batch normalization and ReLU activation\n",
        "        x = torch.mm(x, self.weights1) + self.bias1\n",
        "\n",
        "        if self.training:\n",
        "            self.bn1_mean = x.mean(0, keepdim=True)\n",
        "            self.bn1_var = x.var(0, unbiased=False, keepdim=True)\n",
        "            x = (x - self.bn1_mean) / torch.sqrt(self.bn1_var + 1e-5)\n",
        "        else:\n",
        "            x = (x - self.bn1_running_mean) / torch.sqrt(self.bn1_running_var + 1e-5)\n",
        "\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # Dropout\n",
        "        if self.training:\n",
        "            mask = torch.rand(x.shape) > self.dropout_prob\n",
        "            x = x * mask\n",
        "\n",
        "        # Layer 2\n",
        "        x = torch.mm(x, self.weights2) + self.bias2\n",
        "        return x\n",
        "\n",
        "    def backward(self, x, target, lr):\n",
        "        loss = torch.nn.functional.cross_entropy(x, target)\n",
        "\n",
        "        # Add L2 regularization term to the loss\n",
        "        loss += 0.5 * self.l2_lambda * (torch.sum(self.weights1 ** 2) + torch.sum(self.weights2 ** 2))\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights and biases during training\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                self.weights1 -= lr * self.weights1.grad\n",
        "                self.bias1 -= lr * self.bias1.grad\n",
        "                self.weights2 -= lr * self.weights2.grad\n",
        "                self.bias2 -= lr * self.bias2.grad\n",
        "\n",
        "                # Batch normalization running statistics are updated with a weighted combination of the current running statistics and the batch statistics (bn1_mean and bn1_var) computed during training\n",
        "                self.bn1_running_mean = 0.9 * self.bn1_running_mean + 0.1 * self.bn1_mean\n",
        "                self.bn1_running_var = 0.9 * self.bn1_running_var + 0.1 * self.bn1_var\n",
        "\n",
        "                # Zero the gradients for the next iteration\n",
        "                self.weights1.grad.zero_()\n",
        "                self.bias1.grad.zero_()\n",
        "                self.weights2.grad.zero_()\n",
        "                self.bias2.grad.zero_()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cuda\" if device == \"gpu\" and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loading MNIST dataset\n",
        "train_dataset = datasets.MNIST('data', train=True, download=True, transform=None)\n",
        "validation_dataset = datasets.MNIST('data', train=False, download=True, transform=None)\n",
        "\n",
        "# Initialize the model\n",
        "model = MLP(784, 100, 10)\n",
        "model.weights1 = model.weights1.to(device)\n",
        "model.bias1 = model.bias1.to(device)\n",
        "model.weights2 = model.weights2.to(device)\n",
        "model.bias2 = model.bias2.to(device)\n",
        "\n",
        "# Training loop\n",
        "lr = 0.1\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "train_data = train_dataset.data.float()\n",
        "train_targets = train_dataset.targets\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.training = True  # Set the model in training mode\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    num_batches = len(train_data) // batch_size\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = (batch_idx + 1) * batch_size\n",
        "\n",
        "        data = train_data[start_idx:end_idx].view(-1, 784).to(device)\n",
        "        target = train_targets[start_idx:end_idx].to(device)\n",
        "\n",
        "        output = model.forward(data)\n",
        "        loss = model.backward(output, target, lr)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "        total_loss += loss\n",
        "\n",
        "    # Print training statistics\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Validation loop\n",
        "model.training = False  # Set the model in evaluation mode\n",
        "total_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "validation_data = validation_dataset.data.float()\n",
        "validation_targets = validation_dataset.targets\n",
        "num_validation_batches = len(validation_data) // batch_size\n",
        "\n",
        "for batch_idx in range(num_validation_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = (batch_idx + 1) * batch_size\n",
        "\n",
        "    data = validation_data[start_idx:end_idx].view(-1, 784).to(device)\n",
        "    target = validation_targets[start_idx:end_idx].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model.forward(data)\n",
        "\n",
        "    # Calculate loss and backpropagate\n",
        "    loss = model.backward(output, target, lr)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    _, predicted = output.max(1)\n",
        "    total += target.size(0)\n",
        "    correct += predicted.eq(target).sum().item()\n",
        "    total_loss += loss\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "avg_loss = total_loss / num_validation_batches\n",
        "print(f\"Validation loss: {avg_loss:.4f}, accuracy: {accuracy:.2f}%\")"
      ]
    }
  ]
}