{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nMNZ7sFkj0I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k02A8X3RjVwx"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
        "        # (cuda:1).\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device('mos')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def collate(x) -> Tensor:\n",
        "    if isinstance(x, (tuple, list)):\n",
        "        if isinstance(x[0], Tensor):\n",
        "            return torch.stack(x)\n",
        "        return torch.tensor(x)\n",
        "    raise \"Not supported yet\"\n",
        "\n",
        "def to_one_hot(x: Tensor) -> Tensor:\n",
        "    return torch.eye(x.max() + 1)[x]\n",
        "\n",
        "def load_mnist(path: str = \"./data\", train: bool = True, pin_memory: bool = True):\n",
        "    mnist_raw = MNIST(path, download=True, train=train)\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "    for image, label in mnist_raw:\n",
        "        tensor = torch.from_numpy(np.array(image))\n",
        "        mnist_data.append(tensor)\n",
        "        mnist_labels.append(label)\n",
        "\n",
        "    mnist_data = collate(mnist_data).float()  # shape 60000, 28, 28\n",
        "    mnist_data = mnist_data.flatten(start_dim=1)  # shape 60000, 784\n",
        "    mnist_data /= mnist_data.max()  # min max normalize\n",
        "    mnist_labels = collate(mnist_labels)  # shape 60000\n",
        "    if train:\n",
        "        mnist_labels = to_one_hot(mnist_labels)  # shape 60000, 10\n",
        "    if pin_memory:\n",
        "        return mnist_data.pin_memory(), mnist_labels.pin_memory()\n",
        "    return mnist_data, mnist_labels\n",
        "\n",
        "def activate(x: Tensor) -> Tensor:\n",
        "    print(f\"x: {x.shape}\")\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "def forward(x: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
        "    print(\"forward\")\n",
        "    print(f\"x: {x.shape}\")\n",
        "    print(f\"w: {w.shape}\")\n",
        "    b,wx= torch.broadcast_tensors(b,w @ x)\n",
        "    z=w @ x + b\n",
        "    return z\n",
        "\n",
        "def backward_chain_rule(w: Tensor, w2: Tensor, w3: Tensor,z: Tensor, z2: Tensor, z3: Tensor,a: Tensor, a1: Tensor, a3: Tensor,y):\n",
        "    c = (a3 - y) ** 2\n",
        "    print(f\"c: {c}\")\n",
        "    delta_c=z2*(activate(z3)*(1-activate(z3)))*2*(a3-y)\n",
        "\n",
        "\n",
        "\n",
        "def train_batch(x: Tensor, y: Tensor, w: Tensor, b: Tensor, lr: float) -> Tuple[Tensor, Tensor]:\n",
        "    print(\"train_batch\")\n",
        "    print(f\"x: {x.shape}\")\n",
        "    print(f\"y: {x.shape}\")\n",
        "    print(f\"w: {w.shape}\")\n",
        "    z=forward(x, w, b)\n",
        "    a = activate(z)\n",
        "    w2 = torch.rand((10, 100), device=device)\n",
        "    b2 = torch.rand((10, 1), device=device)\n",
        "    z2=w2 @ z + b2\n",
        "    a2 = activate(z2)\n",
        "    print(f\"a2: {a2.shape}\")\n",
        "    w3 = torch.rand((1, 10), device=device)\n",
        "    b3 = torch.rand((1, 1), device=device)\n",
        "    z3=w3 @ z2 + b3\n",
        "    a3 = activate(z3)\n",
        "    print(f\"a3: {a3.shape}\")\n",
        "    backward_chain_rule(w,w2,w3,z,z2,z3,a,a2,a3,y)\n",
        "\n",
        "    return w, b\n",
        "\n",
        "def train_epoch(x: Tensor, y: Tensor, w: Tensor, b: Tensor, lr: float, batch: int) \\\n",
        "        -> Tuple[Tensor, Tensor]:\n",
        "    non_blocking = w.device.type == 'cuda'\n",
        "    for i in range(0, x.shape[1]):\n",
        "        print(x.shape)\n",
        "        x = x[i:,:i+batch]\n",
        "        y = y[i:,:i+batch]\n",
        "        w, b = train_batch(x, y, w, b, lr)\n",
        "    return w, b\n",
        "\n",
        "def train(epochs,batch, device,x,y):\n",
        "    print(f\"Using device {device}\")\n",
        "    pin_memory = device.type == 'cuda'  # Check the provided references.\n",
        "    w = torch.rand((100, 784), device=device)\n",
        "    b = torch.zeros((100, 1), device=device)\n",
        "    lr = 0.0005\n",
        "    epochs = tqdm(range(epochs))\n",
        "    for _ in epochs:\n",
        "        w, b = train_epoch(x, y, w, b, lr, batch)\n",
        "        accuracy = evaluate(data_test, labels_test, w, b, eval_batch_size)\n",
        "        epochs.set_postfix_str(f\"accuracy = {accuracy}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  device=get_default_device()\n",
        "  pin_memory = device.type == 'cuda'\n",
        "  x, y = load_mnist(train=True, pin_memory=pin_memory)\n",
        "  x=x.swapaxes(0,1)\n",
        "  y=y.swapaxes(0,1)\n",
        "  print(f\"x: {x.shape}\")\n",
        "  print(f\"y: {y.shape}\")\n",
        "  batch=200\n",
        "  epoch=300\n",
        "  train(epoch,batch,device,x,y)"
      ]
    }
  ]
}