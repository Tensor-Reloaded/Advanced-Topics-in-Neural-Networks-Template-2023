{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dy3eyFJ_kda-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vyBq96VVkdbC"
      },
      "outputs": [],
      "source": [
        "MNIST_std = 0.3081\n",
        "MNIST_mean = 0.1307"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eWsi9lgkdbC",
        "outputId": "d5a2bdc6-4ae9-4de9-95c9-d5843635778f"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVR5ic4kkdbD",
        "outputId": "478075fa-fd21-4340-ab6f-4d28eee6a1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before scaling: (tensor(78.5675), tensor(33.3184))\n",
            "After scaling: (tensor(0.3081), tensor(0.1307))\n"
          ]
        }
      ],
      "source": [
        "print(f'Before scaling: {torch.std_mean(train_dataset.data.float())}')\n",
        "print(f'After scaling: {torch.std_mean(train_dataset.data.float() / 255)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WG69mawTkdbF"
      },
      "outputs": [],
      "source": [
        "# Load dataset in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4Sd4EpNekdbF"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + torch.exp(-x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dTWW-dxdkdbG"
      },
      "outputs": [],
      "source": [
        "def softmax(x, dim=1):\n",
        "  exp_x = torch.exp(x)\n",
        "  sum_exp = torch.sum(exp_x, dim=dim, keepdim=True)\n",
        "  return exp_x / sum_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i-5ykjTO2o8u"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return torch.max(torch.zeros_like(x), x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lYltmrqEkdbG"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(labels, num_classes=10):\n",
        "    one_hot = torch.zeros((labels.size(0), num_classes))\n",
        "    one_hot.scatter_(1, labels.unsqueeze(1).long(), 1)\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqnRWArLcYPu"
      },
      "source": [
        "98% accuracy Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0rUPwarkdbH"
      },
      "source": [
        "#### Smart init weights -> random values from a normal distribution with:\n",
        "##### mean = 0\n",
        "##### standard deviation = (1 / sqrt(n)), where n is the total number of connections that go into the neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lOwpx9b8kdbJ"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(output, target):\n",
        "    loss = -torch.sum(target * torch.log(output))\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_gnQP682kdbL"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate, batch_size):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Smart init weights and biases (He initialization)\n",
        "        self.weights_hidden = np.random.normal(0, 2/np.sqrt(input_dim), size=(input_dim, hidden_dim))\n",
        "        self.bias_hidden = np.random.normal(0, 2/np.sqrt(input_dim), size=(hidden_dim))\n",
        "        self.weights_output = np.random.normal(0, 2/np.sqrt(hidden_dim), size=(hidden_dim, output_dim))\n",
        "        self.bias_output = np.random.normal(0, 2/np.sqrt(hidden_dim), size=(output_dim))\n",
        "\n",
        "        self.weights_hidden = torch.from_numpy(self.weights_hidden).to(torch.float32)\n",
        "        self.bias_hidden = torch.from_numpy(self.bias_hidden).to(torch.float32)\n",
        "        self.weights_output = torch.from_numpy(self.weights_output).to(torch.float32)\n",
        "        self.bias_output = torch.from_numpy(self.bias_output).to(torch.float32)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_linear = torch.add(torch.mm(inputs, self.weights_hidden), self.bias_hidden)\n",
        "        hidden_activation = torch.sigmoid(hidden_linear)\n",
        "\n",
        "        output_linear = torch.add(torch.mm(hidden_activation, self.weights_output), self.bias_output)\n",
        "        output_activation = torch.softmax(output_linear, dim=1)\n",
        "\n",
        "        return hidden_activation, output_activation\n",
        "\n",
        "    def backward(self, inputs, hidden_activation, output_activation, targets):\n",
        "        # Gradient of the cross-entropy loss with respect to the output layer\n",
        "        grad_output = output_activation - targets\n",
        "\n",
        "        # Gradient of the hidden layer\n",
        "        grad_hidden = torch.mm(grad_output, self.weights_output.t()) * hidden_activation * (1 - hidden_activation)\n",
        "\n",
        "        # Gradients for weights and biases\n",
        "        grad_weights_output = torch.mm(hidden_activation.t(), grad_output)\n",
        "        grad_bias_output = torch.sum(grad_output, dim=0)\n",
        "        grad_weights_hidden = torch.mm(inputs.t(), grad_hidden)\n",
        "        grad_bias_hidden = torch.sum(grad_hidden, dim=0)\n",
        "\n",
        "        # Update the weights and biases\n",
        "        self.weights_output -= self.learning_rate * grad_weights_output / self.batch_size\n",
        "        self.bias_output -= self.learning_rate * grad_bias_output / self.batch_size\n",
        "        self.weights_hidden -= self.learning_rate * grad_weights_hidden / self.batch_size\n",
        "        self.bias_hidden -= self.learning_rate * grad_bias_hidden / self.batch_size\n",
        "\n",
        "    def train(self, train_loader, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            total_correct = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for inputs, targets in tqdm(train_loader):\n",
        "                # Preprocess inputs and targets\n",
        "                inputs = torch.flatten(inputs, start_dim=1)\n",
        "                targets_one_hot = one_hot_encode(targets, num_classes=self.output_dim).float()\n",
        "\n",
        "                hidden_activation, output_activation = self.forward(inputs)\n",
        "                total_loss += torch.nn.functional.cross_entropy(output_activation, targets_one_hot)\n",
        "                self.backward(inputs, hidden_activation, output_activation, targets_one_hot)\n",
        "\n",
        "                # Compute accuracy for batch\n",
        "                predictions = torch.argmax(output_activation, dim=1)\n",
        "                correct = torch.sum(predictions == targets).item()\n",
        "                total_correct += correct\n",
        "                total_samples += inputs.size(0)\n",
        "\n",
        "            average_loss = total_loss / len(train_loader)\n",
        "            accuracy = total_correct / total_samples\n",
        "\n",
        "            self.evaluate(test_loader)\n",
        "\n",
        "            print(f'Epoch {epoch+1} --- Loss: {average_loss} --- Train acc: {round(accuracy*100)}%')\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        correct = 0\n",
        "        total = len(test_loader.dataset)\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = torch.flatten(inputs, start_dim=1)\n",
        "\n",
        "            _, output_activation = self.forward(inputs)\n",
        "            predictions = torch.argmax(output_activation, dim=1)\n",
        "            correct += torch.sum(predictions == targets).item()\n",
        "        accuracy = correct / total\n",
        "        print(f'Test accuracy: {round(accuracy*100)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_752oJMJERHo"
      },
      "source": [
        "MLP with other methods:\n",
        "- Batch normalization ~ 80%\n",
        "- Adam optimizer ~ 15%\n",
        "- Dropout ~ 90%\n",
        "- relu and log_softmax ~ 97%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4jm0F6sykdbN"
      },
      "outputs": [],
      "source": [
        "# Parameters initialization\n",
        "epochs = 10\n",
        "learning_rate = 0.2\n",
        "input_dim = 784\n",
        "hidden_dim = 100\n",
        "num_classes = 10\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GtLZD1GOkdbN"
      },
      "outputs": [],
      "source": [
        "model = MLP(input_dim, hidden_dim, num_classes, learning_rate, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs9FHVG3kdbN",
        "outputId": "eb011024-27e2-40d7-e0f8-a1804a50185d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 157.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 95%\n",
            "Epoch 1 --- Loss: 1.5970135927200317 --- Train acc: 92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:13<00:00, 136.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 96%\n",
            "Epoch 2 --- Loss: 1.5313067436218262 --- Train acc: 96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:16<00:00, 116.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97%\n",
            "Epoch 3 --- Loss: 1.514066457748413 --- Train acc: 97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:14<00:00, 126.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97%\n",
            "Epoch 4 --- Loss: 1.5044746398925781 --- Train acc: 98%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 159.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97%\n",
            "Epoch 5 --- Loss: 1.4974911212921143 --- Train acc: 98%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 161.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97%\n",
            "Epoch 6 --- Loss: 1.4923949241638184 --- Train acc: 99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 159.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97%\n",
            "Epoch 7 --- Loss: 1.4884897470474243 --- Train acc: 99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 161.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 98%\n",
            "Epoch 8 --- Loss: 1.4849803447723389 --- Train acc: 99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 161.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 98%\n",
            "Epoch 9 --- Loss: 1.4823334217071533 --- Train acc: 99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:11<00:00, 162.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 98%\n",
            "Epoch 10 --- Loss: 1.479832410812378 --- Train acc: 99%\n",
            "Time for training: 2.0m : 144.68344831466675s.\n",
            "Test accuracy: 98%\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "model.train(train_loader, epochs)\n",
        "\n",
        "end = time.time()\n",
        "print(f'Time for training: {(end - start)//60}m : {(end - start)}s.')\n",
        "\n",
        "model.evaluate(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K4SH-ojaKLB"
      },
      "source": [
        "## Compare with torch high-level API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPS8bIMNrMXX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "model = MLP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnfU1avxsWo0",
        "outputId": "87be713f-ebeb-43ce-eb4c-3404091ea056"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [05:06<00:00, 20.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Accuracy of the network on the test images: 0.971700 %\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train\n",
        "for epoch in tqdm(range(15)):\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%' % (correct / total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
