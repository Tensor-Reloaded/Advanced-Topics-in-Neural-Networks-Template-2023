{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Ap9_cCfIrUQB"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
        "        # (cuda:1).\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device('mos')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "def collate(x) -> Tensor:\n",
        "    if isinstance(x, (tuple, list)):\n",
        "        if isinstance(x[0], Tensor):\n",
        "            return torch.stack(x)\n",
        "        return torch.tensor(x)\n",
        "    raise \"Not supported yet\"\n",
        "    # see torch\\utils\\data\\_utils\\collate.py\n",
        "\n",
        "\n",
        "def load_mnist(path: str = \"./data\", train: bool = True, pin_memory: bool = True):\n",
        "    mnist_raw = MNIST(path, download=True, train=train)\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "    for image, label in mnist_raw:\n",
        "        tensor = torch.from_numpy(np.array(image))\n",
        "        mnist_data.append(tensor)\n",
        "        mnist_labels.append(label)\n",
        "\n",
        "    mnist_data = collate(mnist_data).float()  # shape 60000, 28, 28\n",
        "    mnist_data = mnist_data.flatten(start_dim=1)  # shape 60000, 784\n",
        "    mnist_data /= mnist_data.max()  # min max normalize\n",
        "    mnist_labels = collate(mnist_labels)  # shape 60000\n",
        "    if train:\n",
        "        mnist_labels = to_one_hot(mnist_labels)  # shape 60000, 10\n",
        "    if pin_memory:\n",
        "        return mnist_data.pin_memory(), mnist_labels.pin_memory()\n",
        "    return mnist_data, mnist_labels\n",
        "\n",
        "\n",
        "def to_one_hot(x: Tensor) -> Tensor:\n",
        "    return torch.eye(x.max() + 1)[x]\n",
        "\n",
        "\n",
        "def forward(x: Tensor, w: Tensor, b: Tensor) -> Tensor:\n",
        "    return x @ w + b\n",
        "\n",
        "\n",
        "def activate_softmax(x: Tensor) -> Tensor:\n",
        "    return x.softmax(dim=1)\n",
        "\n",
        "\n",
        "def activate_sigmoid(x: Tensor) -> Tensor:\n",
        "    # return 1.0 / (1.0 + torch.exp(-x))\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def train_epoch(data: Tensor, labels: Tensor, w_hidden_layer: Tensor, b_hidden_layer: Tensor, w_last_layer: Tensor, b_last_layer: Tensor, lr: float, batch_size: int) \\\n",
        "        -> Tuple[Tensor, Tensor, Tensor, Tensor, float]:\n",
        "\n",
        "    epoch_loss = 0\n",
        "    non_blocking = w_hidden_layer.device.type == 'cuda'\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        x = data[i: i + batch_size].to(w_hidden_layer.device, non_blocking=non_blocking)\n",
        "        y = labels[i: i + batch_size].to(w_hidden_layer.device, non_blocking=non_blocking)\n",
        "\n",
        "        # forward propagation\n",
        "        hidden_output = activate_sigmoid(forward(x, w_hidden_layer, b_hidden_layer))\n",
        "        last_output = activate_softmax(forward(hidden_output, w_last_layer, b_last_layer))\n",
        "\n",
        "        # loss\n",
        "        epoch_loss += torch.nn.functional.cross_entropy(last_output, y).item()\n",
        "\n",
        "        # backward propagation using chain rule\n",
        "        last_error = last_output - y\n",
        "        delta_w_last = hidden_output.T @ last_error\n",
        "        delta_b_last = last_error.mean(dim=0)\n",
        "\n",
        "        hidden_error = (hidden_output * (1 - hidden_output)) * (w_last_layer @ last_error.T).T\n",
        "        delta_w_hidden = x.T @ hidden_error\n",
        "        delta_b_hidden = hidden_error.mean(dim=0)\n",
        "\n",
        "        w_last_layer -= lr * delta_w_last\n",
        "        b_last_layer -= lr * delta_b_last\n",
        "\n",
        "        w_hidden_layer -= lr * delta_w_hidden\n",
        "        b_hidden_layer -= lr * delta_b_hidden\n",
        "\n",
        "    return w_hidden_layer, b_hidden_layer, w_last_layer, b_last_layer, epoch_loss / batch_size\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(data: Tensor, labels: Tensor, w_hidden_layer: Tensor, b_hidden_layer: Tensor, w_last_layer: Tensor, b_last_layer: Tensor, batch_size: int) -> float:\n",
        "    # Labels are not one hot encoded, because we do not need them as one hot.\n",
        "    total_correct_predictions = 0\n",
        "    total_len = data.shape[0]\n",
        "    non_blocking = w_hidden_layer.device.type == 'cuda'\n",
        "    for i in range(0, total_len, batch_size):\n",
        "        x = data[i: i + batch_size].to(w_hidden_layer, non_blocking=non_blocking)\n",
        "        y = labels[i: i + batch_size].to(w_hidden_layer.device, non_blocking=non_blocking)\n",
        "\n",
        "        hidden_output = activate_sigmoid(forward(x, w_hidden_layer, b_hidden_layer))\n",
        "        last_output = activate_softmax(forward(hidden_output, w_last_layer, b_last_layer))\n",
        "\n",
        "        predicted_max_value, predicted_max_value_indices = torch.max(last_output, dim=1)\n",
        "        # we check if the indices of the max value per line correspond to the correct label. We get a boolean mask\n",
        "        # with True where the indices are the same, false otherwise\n",
        "        equality_mask = predicted_max_value_indices == y\n",
        "        # We sum the boolean mask, and get the number of True values in the mask. We use .item() to get the value out of\n",
        "        # the tensor\n",
        "        correct_predictions = equality_mask.sum().item()\n",
        "        total_correct_predictions += correct_predictions\n",
        "\n",
        "    return total_correct_predictions / total_len\n",
        "\n",
        "\n",
        "def train(epochs: int = 1000, device: torch.device = get_default_device()):\n",
        "    print(f\"Using device {device}\")\n",
        "    pin_memory = device.type == 'cuda'\n",
        "    w_hidden_layer = torch.empty((784, 100), device=device).normal_(mean=0, std=np.power(np.sqrt(784), (-1)))\n",
        "    b_hidden_layer = torch.empty((1, 100), device=device).normal_(mean=0, std=1)\n",
        "\n",
        "    w_last_layer = torch.empty((100, 10), device=device).normal_(mean=0, std=np.power(np.sqrt(100), (-1)))\n",
        "    b_last_layer = torch.empty((1, 10), device=device).normal_(mean=0, std=1)\n",
        "\n",
        "    lr = 0.005\n",
        "    batch_size = 500\n",
        "    eval_batch_size = 500\n",
        "    data, labels = load_mnist(train=True, pin_memory=pin_memory)\n",
        "    data_test, labels_test = load_mnist(train=False, pin_memory=pin_memory)\n",
        "    epochs = tqdm(range(epochs))\n",
        "    total_loss = 0\n",
        "    for _ in epochs:\n",
        "        w_hidden_layer, b_hidden_layer, w_last_layer, b_last_layer, epoch_loss = train_epoch(data, labels, w_hidden_layer, b_hidden_layer, w_last_layer, b_last_layer, lr, batch_size)\n",
        "        total_loss += epoch_loss\n",
        "        accuracy = evaluate(data_test, labels_test, w_hidden_layer, b_hidden_layer, w_last_layer, b_last_layer, eval_batch_size)\n",
        "        epochs.set_postfix_str(f\"accuracy = {accuracy}, epoch loss = {epoch_loss}, total loss = {total_loss}\")\n",
        "        if _ % 100 == 0:\n",
        "            lr *= 0.7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    train(500, torch.device('cpu'))\n",
        "    train(500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2O2aPcSuVgT",
        "outputId": "7b97125b-01df-48f3-86f1-f3f7f0ac905a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:50<00:00,  1.72it/s, accuracy = 0.9701, epoch loss = 0.3513817830085754, total loss = 176.7360966441631]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 291/500 [02:48<01:51,  1.87it/s, accuracy = 0.9673, epoch loss = 0.3520513737201691, total loss = 103.41203617000582]"
          ]
        }
      ]
    }
  ]
}