{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode a batch of labels\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    one_hot = torch.zeros((labels.size(0), num_classes))\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1).long(), 1)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for data, labels in train_loader:\n",
    "    print(data.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + torch.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=1):\n",
    "  exp_x = torch.exp(x)\n",
    "  sum_exp = torch.sum(exp_x, dim=dim, keepdim=True)\n",
    "  return exp_x / sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy on batches\n",
    "def cross_entropy_loss(output, target):\n",
    "  probs = softmax(output, dim=1)\n",
    "\n",
    "  target = target.unsqueeze(1)\n",
    "\n",
    "  loss = -torch.mean(torch.sum(torch.log(probs) * target, dim=1))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_dim, num_classes, learning_rate, loss_fn, batch_size):\n",
    "        self.weights = torch.randn(input_dim, num_classes)\n",
    "        self.bias = torch.randn(num_classes)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        linear = torch.add(torch.mm(inputs, self.weights), self.bias)\n",
    "        return torch.sigmoid(linear)\n",
    "    \n",
    "    def backward(self, inputs, outputs, targets):\n",
    "        grad = outputs - targets\n",
    "\n",
    "        grad_w = torch.mm(inputs.t(), grad)\n",
    "        grad_b = torch.sum(grad, dim=0)\n",
    "\n",
    "        grad_w /= self.batch_size\n",
    "        grad_b /= self.batch_size\n",
    "\n",
    "        self.weights -= self.learning_rate * grad_w\n",
    "        self.bias -= self.learning_rate * grad_b\n",
    "    \n",
    "    def train(self, train_loader, epochs):\n",
    "        for i in range(epochs):\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                # Preprocess inputs and targets\n",
    "                inputs = torch.flatten(inputs, start_dim=1)\n",
    "                targets = one_hot_encode(targets)\n",
    "                \n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                self.backward(inputs, outputs, targets)\n",
    "\n",
    "                # Compute accuracy for batch.\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                targets = torch.argmax(targets, dim=1)\n",
    "                correct = torch.sum((predictions == targets).float())\n",
    "                total_correct += correct\n",
    "                total_samples += len(targets)\n",
    "            print(f'Epoch {i+1} --- Loss: {loss} --- Accuracy: {total_correct/total_samples}')\n",
    "    \n",
    "    def evaluate(self, test_loader):\n",
    "        correct = 0\n",
    "        total = len(test_loader.dataset)\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "            outputs = self.forward(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += torch.sum((predictions == targets).float())\n",
    "        print(f'Accuracy: {correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters initialization\n",
    "epochs = 10\n",
    "learning_rate = 0.15\n",
    "input_dim = 784\n",
    "num_classes = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(input_dim, num_classes, learning_rate, cross_entropy_loss, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- Loss: 7.436459541320801 --- Accuracy: 0.7735000252723694\n",
      "Epoch 2 --- Loss: 7.420828819274902 --- Accuracy: 0.8689166903495789\n",
      "Epoch 3 --- Loss: 7.460615634918213 --- Accuracy: 0.883650004863739\n",
      "Epoch 4 --- Loss: 7.406758785247803 --- Accuracy: 0.8915666937828064\n",
      "Epoch 5 --- Loss: 7.479333400726318 --- Accuracy: 0.896233320236206\n",
      "Epoch 6 --- Loss: 7.500998020172119 --- Accuracy: 0.8999666571617126\n",
      "Epoch 7 --- Loss: 7.475472927093506 --- Accuracy: 0.9023000001907349\n",
      "Epoch 8 --- Loss: 7.433465003967285 --- Accuracy: 0.9050499796867371\n",
      "Epoch 9 --- Loss: 7.430211544036865 --- Accuracy: 0.9074166417121887\n",
      "Epoch 10 --- Loss: 7.486754417419434 --- Accuracy: 0.908133327960968\n",
      "Accuracy: 0.9049999713897705\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, epochs)\n",
    "model.evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
