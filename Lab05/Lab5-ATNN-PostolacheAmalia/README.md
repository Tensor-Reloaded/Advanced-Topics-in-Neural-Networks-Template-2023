Lab5

I have logged using Tensorboard the required hyper-parameters.
The parameters were hyper-tuned using weights and biases. 
Wandb link: https://wandb.ai/ami24amalia/lab5_cifar_10_pipeline
I have attached .png files containing charts of the optimizers' configurations (3 configs for each optimizer).
Chart legend: 'optimizer_learning-rate_batch-size'
Best result: accuracy = 0.4303, optimizer = SDG, learning-rate = 0.09, batch-size = 128
