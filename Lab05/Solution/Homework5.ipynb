{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvKoAOudpeXKirVjvq47nk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baranceanuvlad/Advanced-Topics-in-Neural-Networks-Template-2023/blob/main/Lab05/Solution/Homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f_cQDrWMZK55"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import freeze_support\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "loy_FoGyk8te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login(key=\"d8de0dc8d1a24788568db317fc1c11409ddb2150\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvCPIbr0lEv2",
        "outputId": "4de10f7e-48d1-42a5-de11-3fe94620dba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"random\",\n",
        "    \"name\": \"my-sweep\",\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\"min\": 0.001, \"max\": 0.1},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64, 128]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"adam\"]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQSWmrlqlfCm",
        "outputId": "b4ba0b7b-d81a-4abe-de5e-78b50f4a0996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: pq79aw75\n",
            "Sweep URL: https://wandb.ai/vlapin/uncategorized/sweeps/pq79aw75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
        "        # (cuda:1).\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device('mos')\n",
        "    return torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "l1TG5d1kZPop"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, dataset, cache=True):\n",
        "        if cache:\n",
        "            dataset = tuple([x for x in dataset])\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset[i]"
      ],
      "metadata": {
        "id": "JZvJZMbZZcT6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2,hidden_size_3, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_size_1)\n",
        "        self.fc2 = torch.nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.fc3 = torch.nn.Linear(hidden_size_2, hidden_size_3)\n",
        "        self.fc4 = torch.nn.Linear(hidden_size_3, output_size)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc4(self.relu(self.fc3(self.relu(self.fc2(self.relu(self.fc1(x)))))))\n",
        "        # x = self.fc1(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # return x"
      ],
      "metadata": {
        "id": "CrGBKjdcZc9a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, labels):\n",
        "    fp_plus_fn = torch.logical_not(output == labels).sum().item()\n",
        "    all_elements = len(output)\n",
        "    return (all_elements - fp_plus_fn) / all_elements"
      ],
      "metadata": {
        "id": "0g4oEs60ZjOe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, writer):\n",
        "    model.train()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "    batch_number = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        batch_number += 1\n",
        "\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, labels)\n",
        "        writer.add_scalar(\"Batch Training/Loss\", batch_number, loss)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        output = output.softmax(dim=1).detach().cpu().squeeze()\n",
        "        labels = labels.cpu().squeeze()\n",
        "        all_outputs.append(output)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4) , total_loss / len(train_loader)\n"
      ],
      "metadata": {
        "id": "ckacDUyUZw-B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for data, labels in val_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        output = output.softmax(dim=1).cpu().squeeze()\n",
        "        labels = labels.cpu().squeeze()\n",
        "        all_outputs.append(output)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4), total_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "ldv6x_LfZzJ8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_epoch(model, train_loader, val_loader, criterion, optimizer, device, writer):\n",
        "    acc, loss = train(model, train_loader, criterion, optimizer, device, writer)\n",
        "    acc_val, loss_val = val(model, val_loader, criterion, device)\n",
        "    # torch.cuda.empty_cache()\n",
        "    return acc, acc_val, loss, loss_val"
      ],
      "metadata": {
        "id": "l_OqVklHZ1v7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_norm(model):\n",
        "    norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        norm += torch.norm(param)\n",
        "    return norm\n"
      ],
      "metadata": {
        "id": "2F-26d1rZ305"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(device=get_default_device()):\n",
        "    transforms = [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Resize((28, 28), antialias=True),\n",
        "        v2.Grayscale(),\n",
        "        torch.flatten,\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    data_path = '../data'\n",
        "    train_dataset = CIFAR10(root=data_path, train=True, transform=v2.Compose(transforms), download=True)\n",
        "    val_dataset = CIFAR10(root=data_path, train=False, transform=v2.Compose(transforms), download=True)\n",
        "    train_dataset = CachedDataset(train_dataset)\n",
        "    val_dataset = CachedDataset(val_dataset)\n",
        "\n",
        "\n",
        "    #config = wandb.init()\n",
        "    #learning_rate = config.config.parameters.learning_rate\n",
        "    #batch_size = config.config.parameters.batch_size\n",
        "    #optimizer = config.config.parameters.optimizer\n",
        "\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    model = MLP(784, 1024 ,441, 196, 10)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    epochs = 200\n",
        "\n",
        "    batch_size = 256\n",
        "    val_batch_size = 500\n",
        "    num_workers = 2\n",
        "    persistent_workers = (num_workers != 0)\n",
        "    pin_memory = device.type == 'cuda'\n",
        "    train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=pin_memory, num_workers=num_workers,\n",
        "                              batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
        "    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
        "                            drop_last=False)\n",
        "\n",
        "    log_dir = \"logs\"\n",
        "    writer = SummaryWriter(log_dir)\n",
        "    writer.add_scalar('Batch size', batch_size)\n",
        "    writer.add_text('Optimizer', optimizer.__class__.__name__)\n",
        "    writer.add_scalar('Learning rate', learning_rate)\n",
        "\n",
        "\n",
        "    tbar = tqdm(tuple(range(epochs)))\n",
        "    for epoch in tbar:\n",
        "        acc, acc_val, loss, loss_val = do_epoch(model, train_loader, val_loader, criterion, optimizer, device, writer)\n",
        "        tbar.set_postfix_str(f\"Acc: {acc}, Acc_val: {acc_val}\")\n",
        "        writer.add_scalar(\"Train/Loss\", loss, epoch)\n",
        "        writer.add_scalar(\"Train/Accuracy\", acc, epoch)\n",
        "        writer.add_scalar(\"Val/Loss\", loss_val, epoch)\n",
        "        writer.add_scalar(\"Val/Accuracy\", acc_val, epoch)\n",
        "        writer.add_scalar(\"Model/Norm\", get_model_norm(model), epoch)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YvJA27RtaEnK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    freeze_support()\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ynZAgXcaG4l",
        "outputId": "021a39e0-a7e1-47d7-a5ec-ed95a4512659"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [03:13<00:00,  1.03it/s, Acc: 0.5473, Acc_val: 0.4637]\n"
          ]
        }
      ]
    }
  ]
}